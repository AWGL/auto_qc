import os
import sys
import glob

path_to_data = os.path.join("/","Users","sararey","tmp") # hard code to results directory
dirlist = os.listdir(path_to_data)

run_id = "190524_D00501_0318_BHYMM2BCX2" # need to obtain on the fly
run_id = "190607_M00766_0229_000000000-CCWJY"

run_path = os.path.join(path_to_data,run_id)


def somatic_amplicon(data_path, sample_variables):
    sample_level_logs = False
    vcf = False
    pan = parse_directory(data_path)
    for p in pan: # can do even if one panel only
        # Check for presence of error and output files generated by job scheduler in sample level directories
        print(check_sample_level_logs(p))

        #print(sample_variables) dictionary with samples and contents in variables file

        sample_level_logs = True
        # Check for vcf file generated in sample directory



        # Check for gaps file generated in sample directory



        # Check for bam file generated in sample directory

        vcf = True
    return (sample_level_logs and vcf) # return some indicator of success or a list of fails and why, if failed- dict??


def germline_enrichment(data_path):
    return None


def germline_enrichment_new(data_path):
    return None


def check_results_folder_exists(run_directory):
    try:
        print("Results folder is " + run_directory)
    except:
        raise Exception(f"There is no results folder for run {run_id} also {run_path}")


def load_variables(input_path):
    '''
    :param input_path: the path to the run folder
    :return: dictionary containing the contents of all of the variables file per sample
    '''
    # Parse all panels
    panels_fullpath = (parse_directory(input_path))
    # Create dictionary to hold all the samples as keys
    samples_dict = {}
    # Obtain samples and parse variables files
    for p in panels_fullpath:
        # Parse all samples
        samples_fullpath = parse_directory(p)
        # Iterate through all samples to load contents of variables file into a nested dictionary
        for ps in samples_fullpath:
            samples_name = os.path.basename(ps)
            # Add second key to dictionary
            values_dict = {}
            # Parse variables file
            with open(os.path.join(p, samples_name, f"{samples_name}.variables")) as f:
                for line in f:
                    if not (line.startswith("#") or line.isspace()):
                        divide_line = line.rstrip().split("=")
                        variable = divide_line[0]
                        value = divide_line[1]
                        # Add data to dictionary
                        values_dict[variable] = value
            samples_dict[samples_name] = values_dict
    return samples_dict


def load_variables_archived(input_path):
    '''
    :param input_path: the path to the run folder
    :return: dictionary containing the contents of all of the variables file as a nested dictionary, per sample,
     per panel
    '''
    # Parse all panels
    panels_fullpath = (parse_directory(input_path))
    # Create dictionary to nest data within
    panels_dict = {}
    # Obtain samples and parse variables files
    for p in panels_fullpath:
        # Obtain just panel name without leading path
        panel_name = os.path.os.path.basename(p)
        # Create dictionary to hold all the samples as keys
        samples_dict = {}
        # Parse all samples
        samples_fullpath = parse_directory(p)
        # Iterate through all samples to load contents of variables file into a nested dictionary
        for ps in samples_fullpath:
            samples_name = os.path.basename(ps)
            # Add second key to dictionary
            values_dict = {}
            # Parse variables file
            with open(os.path.join(p, samples_name, f"{samples_name}.variables")) as f:
                for line in f:
                    if not (line.startswith("#") or line.isspace()):
                        divide_line = line.rstrip().split("=")
                        variable = divide_line[0]
                        value = divide_line[1]
                        # Add data to dictionary
                        values_dict[variable] = value
            samples_dict[samples_name] = values_dict
        panels_dict[panel_name] = samples_dict
    return panels_dict


def get_pipeline_name(dict):
    '''
    :param dict: dictionary containing the data from the variables files (output from load_variables)
    :return: list of all pipeline, version combinations present in the dataset
    '''
    pipeline_list = []
    for k in dict:
        pipeline_version= ((dict.get(k).get("pipelineName"), (dict.get(k).get("pipelineVersion"))))
        pipeline_list.append(pipeline_version)
    pipeline_set = set(pipeline_list)
    pipeline_name_samples = list(pipeline_set)
    return pipeline_name_samples


def parse_directory(input_directory):
    '''
    :param input_directory: the full path to the directory to be parsed
    :return: list of directories within the input directory
    '''
    directories_filtered = filter(lambda x: os.path.isdir(os.path.join(input_directory, x)), os.listdir(input_directory))
    directories_path_list = [os.path.join(input_directory, d) for d in directories_filtered]
    return (directories_path_list)


def check_panel_level(path):
    '''
    :param path:
    :return:
    '''
    panels = parse_directory(path)
    print(panels)
    return None


def check_sample_level_logs(path):
    #replace this with a class with optional methods?
    #replace these data structures with a dictionary
    output_log_missing = []
    error_log_missing = []
    vcf_missing = []
    bam_missing = []
    gaps_missing = []
    samples = parse_directory(path)
    for s in samples:
        output_log_missing.append(check_output_log(s))
        error_log_missing.append(check_error_log(s))
        vcf_missing.append(check_vcf_generated(s))
        bam_missing.append(check_bam_generated(s))
        gaps_missing.append(check_gaps_generated(s))
    return [output_log_missing, error_log_missing, vcf_missing]


def check_output_log(loc):
    output_missing = None
    if not (glob.glob(os.path.join(loc, "*.o*"))):
        sample = (os.path.basename(loc))
        output_missing = sample
    return output_missing


def check_error_log(loc):
    output_missing = None
    if not (glob.glob(os.path.join(loc, "*.e*"))):
        sample = (os.path.basename(loc))
        output_missing = sample
    return output_missing


def check_vcf_generated(loc):
    output_missing = None
    if not (glob.glob(os.path.join(loc, "*filtered_meta_annotated.vcf"))):
        sample = (os.path.basename(loc))
        output_missing = sample
    return output_missing

def check_bam_generated(loc):
    output_missing = None
    return output_missing

def check_gaps_generated(loc):
    output_missing = None
    return output_missing

# Current versions of pipeline check?
# Load in file with currently used versions of pipeline

def main(): #pass global variables in?
    # Map pipeline names and versions to functions to call- TODO check the passing of paths to the functions
    pipelines_dict = {('SomaticAmplicon', '1.7.6'): somatic_amplicon,
                        ('SomaticAmplicon', '1.7.7'): somatic_amplicon, # add extra checks for the generated worksheets
                        ('GermlineEnrichment', 'num'): germline_enrichment,
                        ('GermlineEnrichment', 'num'): germline_enrichment_new}


    # Check results folder has been correctly created
    check_results_folder_exists(run_path)

    # load variables file and identify pipeline- assumes that each panel has at least one sample
    try:
        print("Loading variables file")
        variables_dict = load_variables(run_path)
    except:
        raise Exception("Variables file could not be loaded or parsed")

    # Identify pipeline as a list of all pipelines with versions
    pipelines = get_pipeline_name(variables_dict)
    if len(pipelines) >= 1:
        print("Logic to support multiple pipelines here- loop and call appropriate function")
        for pipe in pipelines:
            print(f"Results for pipeline {pipe}")
            print(pipe)
            pipelines_dict.get(pipe, "pipeline not recognised exception throw- manual check")(run_path, variables_dict)
    else:
        raise Exception("No pipelines in variables file")

    # Check run level processing complete if applicable- germline enrichment panels only

    # Check sample level processing complete if applicable

    return None


if __name__ == '__main__':
        main()